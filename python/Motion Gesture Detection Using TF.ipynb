{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_coef = 9\n",
    "batch_size = 10\n",
    "kernel_size = 30\n",
    "depth = 20\n",
    "num_hidden = 100\n",
    "num_channels = 2\n",
    "\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 3\n",
    "\n",
    "filter_value = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve1d(signal, length):\n",
    "    ir = np.ones(length)/length\n",
    "    #return np.convolve(y, ir, mode='same')\n",
    "    \n",
    "    output = np.zeros_like(signal)\n",
    "\n",
    "    for i in range(len(signal)):\n",
    "        for j in range(len(ir)):\n",
    "            if i - j < 0: continue\n",
    "            output[i] += signal[i - j] * ir[j]\n",
    "            \n",
    "    return output\n",
    "\n",
    "def filterRecord(record, filter_value):\n",
    "    x = convolve1d(record[:,0], filter_value)\n",
    "    y = convolve1d(record[:,1], filter_value)\n",
    "    return np.dstack([x,y])[0]\n",
    "\n",
    "\n",
    "def readFileData(file):\n",
    "    column_names = ['timestamp', 'x-axis', 'y-axis', 'z-axis']\n",
    "    data = pd.read_csv(file, header = None, names = column_names)\n",
    "    \n",
    "    x = data[\"x-axis\"]\n",
    "    y = data[\"y-axis\"]\n",
    "    z = data[\"z-axis\"]\n",
    "    \n",
    "    return np.dstack([x,y])[0]\n",
    "\n",
    "def readData(directory):\n",
    "    records = []\n",
    "    labels = np.empty((0))\n",
    "    \n",
    "    allFiles = glob.glob(directory + \"/*.log\")\n",
    "    for file in allFiles:\n",
    "        fileName = os.path.basename(file)\n",
    "        (name, ext) = os.path.splitext(fileName)\n",
    "        parts = name.split(\"_\")\n",
    "        if (len(parts) == 2):\n",
    "            label = parts[0]\n",
    "            fileData = readFileData(file)\n",
    "            \n",
    "            records.append(fileData)\n",
    "            labels = np.append(labels, label)\n",
    "            \n",
    "    return (records, labels)\n",
    "\n",
    "def getRecordsMaxLength(records):\n",
    "    maxLen = 0\n",
    "    for record in records:\n",
    "        if (len(record) > maxLen):\n",
    "            maxLen = len(record)\n",
    "        \n",
    "    return maxLen\n",
    "\n",
    "def extendRecordsLen(records, length):\n",
    "    ret = np.empty((0, length, 2))\n",
    "    \n",
    "    for index in range(len(records)):\n",
    "        record = records[index]\n",
    "        if (len(record) < length):\n",
    "            record = np.pad(record, ((0, length - len(record)), (0,0)), mode='constant', constant_values=0)\n",
    "            \n",
    "        if filter_value != 0: \n",
    "            record = filterRecord(record, filter_value)\n",
    "            \n",
    "        ret = np.append(ret, [record], axis=0)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def augmentRecord(record, shift):\n",
    "    e = np.empty_like(record)\n",
    "    if shift >= 0:\n",
    "        e[:shift] = 0\n",
    "        e[shift:] = record[:-shift]\n",
    "    else:\n",
    "        e[shift:] = 0\n",
    "        e[:shift] = record[-shift:]\n",
    "    return e\n",
    "\n",
    "def augmentData(records, labels, length):\n",
    "    aug_records = np.empty((0, length, 2))\n",
    "    aug_labels = np.empty((0))\n",
    "\n",
    "    for index in range(len(records)):\n",
    "        record = records[index]\n",
    "        label = labels[index]\n",
    "        \n",
    "        aug_records = np.append(aug_records, [record], axis=0)\n",
    "        aug_labels = np.append(aug_labels, label)\n",
    "        \n",
    "        shift = 3 # 3..21 step 3\n",
    "        while shift <= 21:\n",
    "            aug_records = np.append(aug_records, [augmentRecord(record, shift)], axis=0)\n",
    "            aug_labels = np.append(aug_labels, label)\n",
    "            \n",
    "            aug_records = np.append(aug_records, [augmentRecord(record, -shift)], axis=0)\n",
    "            aug_labels = np.append(aug_labels, label)\n",
    "\n",
    "            shift += 3\n",
    "\n",
    "    return (aug_records, aug_labels)\n",
    "        \n",
    "\n",
    "def normalizeRecords(records):\n",
    "    return records / normalization_coef\n",
    "\n",
    "def plotRecord(record, label):\n",
    "    plt.plot(record)\n",
    "    plt.ylabel(label)\n",
    "    plt.show()\n",
    "    \n",
    "def plotRecords(record1, record2):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(record1)\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(record2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record length is 0\n",
      "Samples: 0\n"
     ]
    }
   ],
   "source": [
    "(records, labels) = readData(\"data\")\n",
    "rec_len = getRecordsMaxLength(records)\n",
    "print(\"Record length is %d\" % rec_len)\n",
    "\n",
    "records = extendRecordsLen(records, rec_len)\n",
    "records = normalizeRecords(records)\n",
    "(records, labels) = augmentData(records, labels, rec_len)\n",
    "\n",
    "labelsBin = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "\n",
    "print(\"Samples: %d\" % len(records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-42508b6dc244>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplotRecords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "plotRecords(records[10], records[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = np.random.rand(len(records)) < 0.70\n",
    "train_x = records[train_test_split]\n",
    "train_y = labelsBin[train_test_split]\n",
    "test_x = records[~train_test_split]\n",
    "test_y = labelsBin[~train_test_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(set(labels))\n",
    "total_batches = train_x.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, rec_len, num_channels], name=\"x_input\")\n",
    "X_reshaped = tf.reshape(X, [-1, 1, rec_len, num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X_reshaped, kernel_size, num_channels, depth)\n",
    "p = apply_max_pool(c, 20, 2)\n",
    "c = apply_depthwise_conv(p, 6, depth*num_channels, depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1), f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases, name=\"labels_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    #tf.global_variables_initializer().run()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    # save the graph\n",
    "    tf.train.write_graph(session.graph_def, '.', 'session.pb', False)\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print (\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \", session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print (\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))\n",
    "    saver.save(session, './session.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
